# Job-level configuration options.
job:
  # Type of job to run. Possible values are "train", "eval", and "search". See
  # the corresponding configuration keys for mode information.
  type: train

  # Main device to use for this job (e.g., 'cpu', 'cuda', 'cuda:0')
  device: 'cuda'  

# The seeds of the PRNGs can be set manually for (increased) reproducability.
# Use -1 to use default seed.
random_seed:
  python: -1
  torch: -1
  numpy: -1


## DATASET #####################################################################

dataset:
  # Specify a dataset here. There must be a folder of that name under "data/".
  # If this folder contains a dataset.yaml file, it overrides the defaults
  # specified below.
  name: 'toy'

  # Number of entities. If set to -1, automatically determined from the
  # entity_ids file (see below).
  num_entities: -1

  # Number of relations. If set to -1, automatically determined from the
  # relation_ids file (see below).
  num_relations: -1

  # A list of files associated with this dataset, each associated with a key.
  # Each entry must contain at least the filename and the type fields.
  files:
    # train, valid, and test are the keys used for the splits in training and
    # evaluation.
    #
    # The files are of type "triples" and contain tab-separated fields:
    # - 0: subject index
    # - 1: # relation index
    # - 2: object index
    # - 3-...: arbitrary metadata fields
    #
    # Indexes are assumed to be dense throughout.
    train:
      filename: train.del
      type: triples
    valid:
      filename: valid.del
      type: triples
    test:
      filename: test.del
      type: triples

    # Entity and relation ids files, which store the externally used ids for
    # each entity/relation. These files are optional for many models if
    # 'dataset.num_entitites' and 'dataset.num_relations' is specified.
    #
    # The map format uses two fields that are tab-separated:
    # - 0: internal entity/relation index (as in train/valid/test)
    # - 1: external id (interpreted as string)
    entity_ids:
      filename: entity_ids.del
      type: map
    relation_ids:
      filename: entity_ids.del
      type: map

    # Files that store human-readble string representations of each
    # entity/relation. These files are optional.
    #
    # Type can be map (field 0 is internal index) or idmap (field 0 is external
    # id).
    entity_strings:
      filename: entity_ids.del  # default to id file
      type: map
    relation_strings:
      filename: relation_ids.del  # default to id file
      type: map

    # Additional files can be added as needed
    +++: +++

  # Additional dataset specific keys can be added as needed
  +++: +++

## MODEL #######################################################################

# Which KGE model to use. Examples include "rescal", "complex", "distmult",
# "transe", or "conve". For available models, see the project homepage and/or
# configuration files under kge/models and kge/models/experimental.
model: ''


## TRAINING ####################################################################

# Options of training jobs (job.type=="train")
train:
  # Type of training job.
  # - KvsAll: scores each unique sp/po pair along with all possible objectes/subjects.
  # - negative_sampling: scores each unique spo triple along with sampled corrupted
  #   triples
  # - 1vsAll: scores each spo triples against the complete set of s/p-corrputed triples
  #   (all treated negative)
  type: KvsAll

  # Loss used for training. Not all combination of losses and job types are supported.
  # - bce: binary cross entropy after taking logits (all training types)
  # - kl: KL divergence between model and data distribution (all training types)
  # - margin_ranking (negative_sampling only)
  loss: bce

  ## Argument of loss function (if any)
  ## - margin_ranking: the margin to use
  loss_arg: 1.0

  # Maximum number of epochs used for training
  max_epochs: 20

  # Batch size used for training.
  batch_size: 100

  # Number of workers used to construct batches. Leave at 0 for default.
  num_workers: 0

  # Optimizer used for training.
  optimizer: Adagrad           # sgd, adagrad, adam

  # Additional arguments for the optimizer. Arbitrary key-value pairs can be
  # added here and will be passed along to the optimizer. E.g., use entry lr:0.1
  # to set the learning rate to 0.1.
  optimizer_args:
    +++: +++

  # Learning rate scheduler to use. Any scheduler from torch.optim.lr_scheduler
  # can be used (e.g., ConstantLRScheduler or ReduceLROnPlateau).
  lr_scheduler: ConstantLRScheduler #

  # Additional arguments for the scheduler.
  lr_scheduler_args:
    +++: +++

  # When to write entries to the trace file.
  trace_level: epoch           # batch, epoch

  # When to create checkpoints
  checkpoint:
    # In addition the the checkpoint of the last epoch (which is transient),
    # create an additional checkpoint every this many epochs. Disable additional
    # checkpoints with 0.
    every: 5

    # Keep this many most recent additional checkpoints.
    keep: 3

  # Other options
  pin_memory: False
  auto_correct: False         # True: allows for trainer to adjust certain settings, False: throw exception
  visualize_graph: False      # create PDF of compute graph (of first batch of first epoch)


# Options for KvsAll training (train.type=="KvsAll)
KvsAll:
  # Amount of label smoothing (disabled when 0). Disencourages models to
  # perform extreme predictions (0 or 1).
  #
  # Technically, reduces all labels by fraction given by this value and
  # subsequently increases them by 1.0/num_entities. For example, 0s become
  # 1.0/num_entities and 1s become (1.0-label_smoothing)+1.0/num_entities.
  #
  # This form of label smoothing was used by ConvE
  # ( https://github.com/TimDettmers/ConvE/blob/853d7010a4b569c3d24c9e6eeaf9dc3535d78832/main.py#L156) with a default value of 0.1.
  label_smoothing: 0.0

# Options for negative sampling training (train.type=="negative_sampling")
negative_sampling:
  sampling_type: uniform
  num_samples_s: 3          # this is default value if others are -1, must not be -1
  num_samples_p: -1
  num_samples_o: -1
  filter_positives_s: False        # default is not filtering
  filter_positives_p: False
  filter_positives_o: False

  # set to spo (for small negative samples), sp_po (for large negative samples),
  # or set to auto which sets spo or sp_po automatically if the
  # max(num_samples_s, num_samples_p, num_samples_o) > 30. Recommended for
  # models which compute sp_po with an inner product, but not for all other
  # models (e.g., not for TransE in the current implementation).
  implementation: spo


## VALIDATION ##################################################################

# Configuration options for model validation/selection during training.
valid:
  # Validation is run every this many epochs during training (disable validation
  # with 0).
  every: 5

  # Name of the trace entry that holds the validation metric (higher value is
  # better)
  metric: mean_reciprocal_rank_filtered

  # If the above metric is not present in trace (e.g., because a custom metric
  # should be used), a Python expression to compute the metric. Can refer to
  # trace entries directly and to configuration options via config.
  # Example: 'math.sqrt(mean_reciprocal_rank) + config.get("user.par")'
  metric_expr: 'float("nan")'

  early_stopping:
    # Grace period of validation runs before a training run is stopped early
    # (disable early stopping with 0). If the value is set to n, then training is
    # stopped when there has been no improvement (compared to the best overall
    # result so far) in the validation metric during the last n validation runs.
    patience: 5

    # A minimum validation metric value threshold that should be reached after
    # n epochs, set to 0 epoch to turn off. Should be set very very conservatively
    # and the main purpose is for pruning completely useless hyperparameter
    # settings during hyper-parameter optimization.
    min_threshold:
      epochs: 0
      metric_value: 0.0

  # When to write a trace entry. Possible values: example, batch, epoch
  trace_level: epoch

  # Whether test data should used during validation. Filtered
  # metrics by default only use train and validation data. When this is set to
  # True, additionally produces "filtered_with_test" validation metrics (such as
  # MRR or HITS@k). Apparently, many existing models have been trained with this
  # set to True during model selection and using a metric such as
  # mean_reciprocal_rank_filtered_with_test.
  filter_with_test: False


## EVALUATION ##################################################################

# Options of evaluation jobs (job.type=="eval"). Also used during validation
# when training. Right now, evaluation jobs compute a fixed set of metrics: the
# MRR and the specified HITS@k's.
eval:
  data: valid
  type: entity_ranking
  hits_at_k_s: [1, 3, 10, 50, 100, 200, 300, 400, 500, 1000]     # ks for HITS@k
  batch_size: 100
  num_workers: 0
  pin_memory: False
  trace_level: example            # example or batch or epoch
  chunk_size: -1                  # default no chunking

  # Metrics are always computed over the entire evaluation data. Optionally,
  # certain more specific metric can be computed in addition.
  metrics_per:
    relation_type: False          # 1-to-1, 1-to-N, N-to-1, N-to-N
    head_and_tail: False          # head, tail
    argument_frequency: False     # 25%, 50%, 75%, top quantiles per argument


## HYPERPARAMETER SEARCH #######################################################

# Options of hyperparameter search jobs (job.type=="search").
search:
  # The type of search to run (see descriptions below). Possible values: manual,
  # grid, ax
  type: ax

  # Maximum number of parallel training jobs to run during a search.
  num_workers: 1

  # Device pool to use for training jobs. If this list is empty, `job.device` is
  # used for all parallel searches. Otherwise, the first `search.num_workers`
  # devices from this list are used. If the number of devices specified here is
  # less than `search.num_workers`, the list wraps around so that devices are
  # used by multiple jobs in parallel.
  device_pool: [ ]

  # What to do when an error occurs during a training job. Possible values:
  # continue, abort
  on_error: abort

# Manually specify all configurations to try
manual_search:
  # If false, only creates training job folders but does not run the jobs.
  run: True

  # List of configurations to search. Each entry is a record with a field
  # 'folder' (where the training job is stored) and an arbitrary number of other
  # fields that define the search configuration (e.g.
  # 'train.optimizer_args.lr').
  configurations: []


# Metajob for a grid search. Creates a manual search job with all points on the
# grid.
grid_search:
  # If false, only creates manual search job configuration file but does not run
  # it. This may be useful to edit the configuration (e.g., change folder names)
  # or to copy the created configurations to an existing manual search job.
  run: True

  # Define the grid. This is a dict where the key is a (flattened or nested)
  # configuration option (e.g., "train.optimizer_args.lr") and the value is an
  # array of grid-search values (e.g., [ 0.1, 0.01 ]). No default values
  # specified.
  parameters:
    +++: +++

# Dynamic search job that picks configurations using ax
ax_search:
  # Total number of trials to run. Can be increased when a search job is
  # resumed.
  num_trials: 10

  # Number of sobol trials to run (-1: automatic); remaining trials are GP+EI.
  # If equal or larger than num_trials, only Sobal trials will be run.
  num_sobol_trials: -1

  # Search space definition passed to ax. See create_experiment in
  # https://ax.dev/api/service.html#module-ax.service.ax_client
  parameters: []
  parameter_constraints: []

  # 'fixed_parameters' is a dictionary of parameters and the values these
  # parameters should be fixed to for *generating* trials during BO. The purpose
  # of 'fixed_parameters' is to initialize a search (f.ex. with Sobol) and then
  # fix a range or choice parameter when generating new trials with BO to a
  # certain value. The names and values should correspond with the definitions
  # in 'ax_search.parameters'.
  # Attention: there is a name clash with regards to the meaning of 'fixed' in Ax
  # and Botorch! The difference to 'ax_search.parameters#type = fixed' is, that
  # those are filtered out for fitting a BO model, while the parameters for
  # 'fixed_parameters' are not. This setting is used in /botorch/gen.py
  #
  # Example:
  #
  #  parameters:
  #    - name: rescal.entity_embedder.dim
  #      type: choice
  #      values: [100, 200]
  #
  #  fixed_parameters:
  #    - name: rescal.entity_embedder.dim
  #      value: 100

  fixed_parameters: []

visualize:
  #visdom, tensorboard
  module: visdom

  # metrics to be visualized in post/broadcast; accepts regex
  # leave empty to select all metrics in the trace (can be slow; when windows overlap in visdom press the repack button or scroll)
  # currently, values that are not float or int types will be ignored
  include_train: ["avg_loss", "avg_cost", "avg_penalty", "forward_time", "epoch_time"]
  include_eval: ["rank"]

  # metrics to be excluded, accepts regex; leaving empty will exclude none
  # has priority over include_train/eval e. g. whatever is in these lists
  # will not be visualized. Useful usecase: leave include_train/eval empty and then filter out some metrics with
  # exclude_train/eval
  exclude_train: ["avg_loss"]
  exclude_eval: []

  # set True; only needed for debugging
  surpress_server_output: True

  # set broadcast.enable=True for broadcasting training/search data of an execution
  # of the kge framework. Broadcast is only supported with module:visdom
  broadcast:
    enable: False

  # post execution visualizations of the jobs can be obtained with running:
  # "python kge.py visualize visualize-options.yaml"
  post:
    # folders in local/experiments/ to be visualized; leave empty for all
    # suggested for visdom: leave empty; for tensorboard: select specific
    folders: []

    # embeddings currently only supported with module:tensorboard
    embeddings: False


## USER PARAMETERS #####################################################################

# These parameters are not used by the kge framework itself. It can be used to
# add additional configuration options or information to this config.
user:
  +++: +++
