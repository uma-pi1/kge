job:
  type: train                 # train, eval, grid, ...
  device: 'cuda'

## INPUT/OUTPUT ################################################################

dataset:
  name: 'toy'

  # training, validation, and test data files have the following fields, all
  # tab-separated:
  # - 0: subject index
  # - 1: relation index
  # - 2: object index
  # - 3-...: arbitrary metadata fields
  # Indexes are assumed to be dense throughout.
  train: train.del
  valid: valid.del
  test: test.del

  # entity and relation maps store auxiliary information about each
  # entity/predicate. Fields are tab-separated:
  # - 0: entity/predicate index (as in train/valid/test)
  # - 1...: arbitrary metadata fields
  entity_map: entity_map.del
  relation_map: relation_map.del

## MODEL #######################################################################

# see model specific configuration files
model: ''                     # distmult, complex, transe, conve, ...


## TRAINING ####################################################################
## Used for training jobs.

train:
  type: 1toN                  # 1toN, negative_sampling
  max_epochs: 20
  optimizer: Adagrad          # sgd, adagrad, adam
  optimizer_args:
    # No default arguments are specified. Here, +++ signals that this option
    # allows arbitrary additional key-value pairs.
    +++: +++
  lr_scheduler: ConstantLRScheduler # schdulers from torch.optim.lr_scheduler such as ReduceLROnPlateau
  lr_scheduler_args:
    +++: +++
  batch_size: 100
  loss: bce                   # bce, margin_ranking
  loss_arg: 1.0               # margin for margin_ranking
  num_workers: 0
  pin_memory: False
  trace_level: epoch          # batch or epoch

1toN:
  # Amount of label smoothing (disabled when <0). Disencourages models to
  # perform extreme predictions (0 or 1).
  #
  # Technically, reduces all labels by fraction given by this value and
  # subsequently increases them by 1.0/num_entities. For example, 0s become
  # 1.0/num_entities and 1s become (1.0-label_smoothing)+1.0/num_entities.
  #
  # This form of label smoothing was used by ConvE
  # ( https://github.com/TimDettmers/ConvE/blob/853d7010a4b569c3d24c9e6eeaf9dc3535d78832/main.py#L156) with a default value of 0.1.
  label_smoothing: -1.0

negative_sampling:
  sampling_type: uniform
  num_negatives_s: 3          # this is default value if others are -1, must not be -1
  num_negatives_p: 0          
  num_negatives_o: -1

# Configuration options for model validation/selection during training.
valid:
  # Validation is run every this many epochs during training (disable validation
  # with 0).
  every: 5

  # Name of the trace entry that holds the validation metric (higher value is
  # better)
  metric: mean_reciprocal_rank_filtered

  # If the above metric is not present in trace (e.g., because a custom metric
  # should be used), a Python expression to compute the metric. Can refer to
  # trace entries directly and to configuration options via config.
  # Example: 'math.sqrt(mean_reciprocal_rank) + config.get("user.par")'
  metric_expr: ''

  early_stopping:
    # Grace period of validation runs before a training run is stopped early
    # (disable early stopping with 0). If the value is set to n, then training is
    # stopped when there has been no improvement (compared to the best overall
    # result so far) in the validation metric during the last n validation runs.
    patience: 5
    # A minimum validation metric value threshold that should be reached after
    # n epochs, set to 0 epoch to turn off. Should be set very very conservatively
    # and the main purpose is for pruning completely useless hyperparameter
    # settings hyper-parameter optimization.
    min_threshold:
      epochs: 0
      metric_value: 0.0

  # When to write a trace entry. Possible values: example, batch, epoch
  trace_level: epoch

  # Whether test data should be leaked into validation (seriously!). Filtered
  # metrics by default only use train and validation data. When this is set to
  # True, additionally produces "filtered_with_test" validation metrics (such as
  # MRR or HITS@k). Apparently, many existing models have been trained with this
  # set to True during model selection and using a metric such as
  # mean_reciprocal_rank_filtered_with_test.
  filter_with_test: False

checkpoint:
  every: 5                    # epochs (disable with 0)

## EVALUATION ##################################################################
## Used for evaluation jobs.

# fixed metrics: compute MRR and HITS@1, ..., HITS@k
eval:
  data: valid
  type: entity_ranking
  max_k: 10                       # maximum k for HITS@k
  batch_size: 100
  num_workers: 0
  pin_memory: False
  trace_level: example            # example or batch or epoch

## HYPERPARAMETER SEARCH ###############################################################

search:
  # the type of search to run (see descriptions below): manual, random, grid, ax, hyperband,
  # bohb, tpe
  type: random

  # (maximum) number of parallel training jobs to run during a search
  num_workers: 1

# Manually specify all configurations to try
manual_search:
  # If false, only creates training job folders but does not run the jobs.
  run: True

  # List of configurations to search. Each entry is a record with a field
  # 'folder' (where the training job is stored) and an arbitrary number of other
  # fields that define the search configuration (e.g.
  # 'train.optimizer_args.lr').
  configurations: []

# Dynamic search job that picks configurations using random search
random_search:
  num_train_trials: 1 # This is not the number of HPO trials!
  max_evals: 10 # This is the number of HPO trials
  parameters: []
# Possible parameter alternatives and choices
#  -name: xx
#   type: [fixed, range, choice]
#   value: x                      # (if type=fixed)
#   bounds: [lower, upper]        # (if type=range)
#   values: [choice1, choice2]    # (if type=choice)
#   Choices should be in the form of hyperopt parameters
#   # Can only have an integer_space OR be log_scale
#   integer_space: 4     # (not necessary) Natural number > 0, if parameter is integer
#   log_scale: True      # (not neccessary) boolean
#
#   Selected Hyperopt Parameters:
#   {'name': hp.quniform('name', lower_bound, upper_bound, spacing)}   # For integer values
#   {'name': hp.uniform('name', lower_bound, upper_bound)}             # For real values
#   {'name': hp.choice('name', [choice1, choice2]}                     # For choices


# Metajob for a grid search. Creates a manual search job with all points on the
# grid.
grid_search:
  # If false, only creates manual search job configuration file but does not run
  # it. This may be useful to edit the configuration (e.g., change folder names)
  # or to copy the created configurations to an existing manual search job.
  run: True

  # Define the grid. This is a dict where the key is a (flattened or nested)
  # configuration option (e.g., "train.optimizer_args.lr") and the value is an
  # array of grid-search values (e.g., [ 0.1, 0.01 ]). No default values
  # specified.
  parameters:
    +++: +++

# Dynamic search job that picks configurations using ax
ax_search:
  num_trials: 10

  # Search space definition passed to ax. See create_experiment in
  # https://ax.dev/api/service.html#module-ax.service.ax_client
  parameters: []
  parameter_constraints: []


#TODO Hyperband Parameters
# Dynamic search job that uses a bandit approach and picks hyperparameters at random
hyperband_search:
  run_id: 'example'
  host: '127.0.0.1'
  port: "None"
  sleep_interval: 0
  min_budget: 1
  max_budget: 10
  n_iter: 5
  num_train_trials: 1
  parameters: []
# Possible hyperparameter choices and alternatives
#    - name:              # string, mandatory
#      type:              # [ordinal, categorical, integer, float] mandatory
#      uniform            # bool, mandatory for integer and float
#      log_scale          # bool, mandatory for integer and float
#      bounds             # [%f, %f], mandatory if uniform = True
#      mu                 # float, mandatory if uniform = False, mean of normal distribution
#      sigma              # float, mandatory if uniform = False, std. div. of normal distribution
#      choices            # list of strings, mandatory if type = categorical
#      values             # list of floats, mandatory if type = ordinal


#TODO BOHB Parameters
# Dynamic search job that uses a bandit approach and picks hyperparameters at random using bayesian statistics
bohb_search:
  run_id: 'example'
  host: '127.0.0.1'
  port: "None"
  sleep_interval: 0
  min_budget: 1
  max_budget: 10
  n_iter: 5
  num_train_trials: 1
  parameters: []
# Possible hyperparameter choices and alternatives as for HyperBand


# Dynamic search job that picks configurations using Tree Parzen Estimators
tpe_search:
  num_train_trials: 1 # This is not the number of HPO trials!
  max_evals: 10 # This is the number of HPO trials
  parameters: []
# Possibilities and alternatives are the same as in random search

## USER PARAMETERS #####################################################################

# These parameters are not used by the kge framework itself. It can be used to
# add additional configuration options or information to this config.
user:
  +++: +++
